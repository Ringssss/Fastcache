#!/usr/bin/env python3
"""
æç«¯å†…å­˜ç“¶é¢ˆæµ‹è¯•ï¼šè®©å‹ç¼©ååè¶…è¿‡30%
=====================================

æ ¸å¿ƒç­–ç•¥ï¼š
1. é•¿promptï¼ˆå ç”¨æ›´å¤šåˆå§‹blocksï¼‰
2. é•¿è¾“å‡ºï¼ˆè®©è¯·æ±‚é•¿æ—¶é—´åœ¨decodeé˜¶æ®µï¼‰
3. å¤§é‡è¯·æ±‚ï¼ˆè¶…è¿‡æ— å‹ç¼©å¹¶å‘èƒ½åŠ›ï¼‰

è¿™æ ·ï¼š
- æ— å‹ç¼©ï¼šåªèƒ½åŒæ—¶decode Nä¸ªè¯·æ±‚ï¼Œå…¶ä»–å¿…é¡»ç­‰å¾…
- æœ‰å‹ç¼©ï¼šå¯ä»¥åŒæ—¶decode 4-5Nä¸ªè¯·æ±‚ï¼Œå¤§å¹…æå‡åå

"""

from fastcache_paths import ensure_sys_paths, CKPT_DIR, DATASETS_DIR, RESULTS_DIR

ensure_sys_paths()

import os
import sys

import torch
import gc
import time
from typing import List


def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


def generate_prompts(num: int, target_tokens: int) -> List[str]:
    base = "USER: Please explain "
    topics = ["AI", "ML", "DL", "NLP", "CV", "RL", "robotics", "quantum", "blockchain", "cloud"]
    expansion = " in great detail covering history, applications, challenges, and future. "

    prompts = []
    repeat = target_tokens // 30
    for i in range(num):
        topic = topics[i % len(topics)]
        prompt = base + topic + expansion * repeat + " ASSISTANT:"
        prompts.append(prompt)
    return prompts


def test_extreme_memory_pressure(model_path: str, compressor_path: str):
    """æç«¯å†…å­˜ç“¶é¢ˆæµ‹è¯•"""
    from nanovllm.sampling_params import SamplingParams
    from nanovllm.engine.llava_engine import LlavaLLM

    print("\n" + "=" * 70)
    print(" æç«¯å†…å­˜ç“¶é¢ˆæµ‹è¯•")
    print("=" * 70)

    # é…ç½®ï¼šè¶…é•¿prompt + è¶…é•¿è¾“å‡º + å¤§é‡è¯·æ±‚
    num_requests = 400
    prompt_tokens = 2000  # éå¸¸é•¿çš„prompt
    max_output = 128  # é•¿è¾“å‡º

    prompts = generate_prompts(num_requests, prompt_tokens)

    # =====================================================
    # æµ‹è¯•1ï¼šæ— å‹ç¼©
    # =====================================================
    print("\n--- æ— å‹ç¼© ---")
    clear_gpu()

    llm = LlavaLLM(
        model_path,
        enable_compression=False,
        enforce_eager=True,
        max_model_len=4096,
    )

    block_size = llm.scheduler.block_manager.block_size
    total_blocks = len(llm.scheduler.block_manager.blocks)
    sample_len = len(llm.tokenizer.encode(prompts[0]))
    blocks_per_prompt = (sample_len + block_size - 1) // block_size
    blocks_per_output = (max_output + block_size - 1) // block_size
    total_blocks_per_req = blocks_per_prompt + blocks_per_output
    max_concurrent = total_blocks // total_blocks_per_req

    print(f"é…ç½®:")
    print(f"  æ€»blocks: {total_blocks}")
    print(f"  æ¯ä¸ªprompt: {sample_len} tokens = {blocks_per_prompt} blocks")
    print(f"  æ¯ä¸ªè¾“å‡º: {max_output} tokens = {blocks_per_output} blocks")
    print(f"  æ¯ä¸ªè¯·æ±‚æ€»è®¡: {total_blocks_per_req} blocks")
    print(f"  ç†è®ºæœ€å¤§å¹¶å‘(æ— å‹ç¼©): {max_concurrent}")

    # æ·»åŠ æ‰€æœ‰è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output))

    # è¿è¡Œ
    total_output_tokens = 0
    start = time.time()
    completed = 0

    while not llm.is_finished():
        outputs, num_tokens = llm.step(apply_compression=False)
        if num_tokens < 0:
            total_output_tokens += (-num_tokens)
        completed += len(outputs)

        # æ¯5ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
        if int(time.time() - start) % 5 == 0 and int(time.time() - start) > 0:
            free = len(llm.scheduler.block_manager.free_block_ids)
            running = len(llm.scheduler.running)
            waiting = len(llm.scheduler.waiting)
            elapsed = time.time() - start
            throughput = total_output_tokens / elapsed if elapsed > 0 else 0
            print(f"  [{elapsed:.0f}s] running={running}, waiting={waiting}, "
                  f"completed={completed}, throughput={throughput:.0f} tok/s")

    no_compress_time = time.time() - start
    no_compress_throughput = total_output_tokens / no_compress_time

    print(f"\næ— å‹ç¼©ç»“æœ:")
    print(f"  æ€»è¾“å‡ºtokens: {total_output_tokens}")
    print(f"  æ€»æ—¶é—´: {no_compress_time:.2f}s")
    print(f"  è¾“å‡ºåå: {no_compress_throughput:.1f} tok/s")

    del llm
    clear_gpu()

    # =====================================================
    # æµ‹è¯•2ï¼šæœ‰å‹ç¼©
    # =====================================================
    print("\n--- æœ‰å‹ç¼© ---")

    llm = LlavaLLM(
        model_path,
        compressor_path=compressor_path,
        enable_compression=True,
        async_compression=False,
        compression_factor=5,
        enforce_eager=True,
        max_model_len=4096,
    )

    blocks_per_compressed = (sample_len // 5 + block_size - 1) // block_size
    total_blocks_compressed = blocks_per_compressed + blocks_per_output
    max_concurrent_compress = total_blocks // total_blocks_compressed

    print(f"é…ç½®(å‹ç¼©å):")
    print(f"  å‹ç¼©åprompt blocks: {blocks_per_compressed}")
    print(f"  æ¯ä¸ªè¯·æ±‚æ€»è®¡: {total_blocks_compressed} blocks")
    print(f"  ç†è®ºæœ€å¤§å¹¶å‘(å‹ç¼©å): {max_concurrent_compress}")
    print(f"  å¹¶å‘èƒ½åŠ›æå‡: {max_concurrent_compress / max_concurrent:.1f}x")

    # æ·»åŠ æ‰€æœ‰è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output))

    # è¿è¡Œ
    total_output_tokens = 0
    start = time.time()
    completed = 0

    while not llm.is_finished():
        outputs, num_tokens = llm.step(apply_compression=True)
        if num_tokens < 0:
            total_output_tokens += (-num_tokens)
        completed += len(outputs)

        # æ¯5ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
        elapsed = time.time() - start
        if int(elapsed) % 5 == 0 and int(elapsed) > 0:
            free = len(llm.scheduler.block_manager.free_block_ids)
            running = len(llm.scheduler.running)
            waiting = len(llm.scheduler.waiting)
            throughput = total_output_tokens / elapsed if elapsed > 0 else 0
            print(f"  [{elapsed:.0f}s] running={running}, waiting={waiting}, "
                  f"completed={completed}, throughput={throughput:.0f} tok/s")

    compress_time = time.time() - start
    compress_throughput = total_output_tokens / compress_time

    print(f"\næœ‰å‹ç¼©ç»“æœ:")
    print(f"  æ€»è¾“å‡ºtokens: {total_output_tokens}")
    print(f"  æ€»æ—¶é—´: {compress_time:.2f}s")
    print(f"  è¾“å‡ºåå: {compress_throughput:.1f} tok/s")

    del llm
    clear_gpu()

    # =====================================================
    # å¯¹æ¯”
    # =====================================================
    print("\n" + "=" * 70)
    print(" æœ€ç»ˆå¯¹æ¯”")
    print("=" * 70)

    speedup = compress_throughput / no_compress_throughput
    improvement = (speedup - 1) * 100

    print(f"\næ— å‹ç¼©åå: {no_compress_throughput:.1f} tok/s")
    print(f"æœ‰å‹ç¼©åå: {compress_throughput:.1f} tok/s")
    print(f"")
    print(f"ğŸ¯ ååæå‡: {improvement:.1f}%")
    print(f"")

    if improvement >= 30:
        print(f"âœ… æˆåŠŸï¼å‹ç¼©å¸¦æ¥ {improvement:.1f}% ååæå‡ (è¶…è¿‡30%ç›®æ ‡)")
    elif improvement >= 0:
        print(f"âš ï¸ ååæå‡ {improvement:.1f}% (æœªè¾¾åˆ°30%ç›®æ ‡)")
    else:
        print(f"âŒ ååä¸‹é™ {-improvement:.1f}%")


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='/data/huggingface/llava-1.5-7b-hf')
    parser.add_argument('--compressor', default=str(CKPT_DIR / "llava_mlp.pth"))
    args = parser.parse_args()

    print("#" * 70)
    print(" æç«¯å†…å­˜ç“¶é¢ˆæµ‹è¯• - ç›®æ ‡: å‹ç¼©ååè¶…è¿‡30%")
    print("#" * 70)

    test_extreme_memory_pressure(args.model, args.compressor)

    print("\nâœ“ æµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    main()
