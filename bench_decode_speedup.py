#!/usr/bin/env python3
"""
Decodeé˜¶æ®µåŠ é€Ÿæµ‹è¯•ï¼šéªŒè¯å‹ç¼©åœ¨å¤§batch decodeä¸‹çš„ååæå‡
===========================================================

æ ¸å¿ƒæ€è·¯ï¼š
- å‹ç¼©çš„çœŸæ­£ä»·å€¼æ˜¯å‡å°‘decodeé˜¶æ®µçš„attentionè®¡ç®—é‡
- å¦‚æœKV-cacheä»2000â†’400 tokensï¼Œattentionè®¡ç®—é‡å‡å°‘5x
- éœ€è¦é•¿è¾“å‡ºè®©decodeé˜¶æ®µå ä¸»å¯¼ï¼Œæ‰èƒ½ä½“ç°å‹ç¼©ä¼˜åŠ¿

æµ‹è¯•é…ç½®ï¼š
- é•¿promptï¼ˆ1000-1500 tokensï¼‰
- é•¿è¾“å‡ºï¼ˆ512-1024 tokensï¼‰
- å¤§batch sizeï¼ˆ128/256ï¼‰

"""

from fastcache_paths import ensure_sys_paths, CKPT_DIR, DATASETS_DIR, RESULTS_DIR

ensure_sys_paths()

import os
import sys

import torch
import gc
import time
from typing import List


def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


def generate_prompts(num: int, target_tokens: int) -> List[str]:
    """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„prompts"""
    base = "USER: Please explain "
    topics = ["AI", "ML", "DL", "NLP", "CV", "RL", "robotics", "quantum", "blockchain", "cloud"]
    expansion = " in great detail covering history, applications, challenges, and future. "

    prompts = []
    repeat = target_tokens // 30
    for i in range(num):
        topic = topics[i % len(topics)]
        prompt = base + topic + expansion * repeat + " ASSISTANT:"
        prompts.append(prompt)
    return prompts


def test_decode_speedup(
    model_path: str,
    compressor_path: str,
    batch_size: int = 128,
    prompt_tokens: int = 1000,
    max_output: int = 512
):
    """
    æµ‹è¯•decodeé˜¶æ®µçš„ååæå‡

    å…³é”®ï¼šé•¿è¾“å‡ºè®©decodeå ä¸»å¯¼ï¼Œä½“ç°å‹ç¼©å¯¹attentionè®¡ç®—çš„å‡å°‘
    """
    from nanovllm.sampling_params import SamplingParams
    from nanovllm.engine.llava_engine import LlavaLLM

    print("\n" + "=" * 70)
    print(f" DecodeåŠ é€Ÿæµ‹è¯•: BS={batch_size}, Prompt={prompt_tokens}, Output={max_output}")
    print("=" * 70)

    prompts = generate_prompts(batch_size, prompt_tokens)

    # =====================================================
    # æµ‹è¯•1ï¼šæ— å‹ç¼©
    # =====================================================
    print("\n--- æ— å‹ç¼©åŸºçº¿ ---")
    clear_gpu()

    llm = LlavaLLM(
        model_path,
        enable_compression=False,
        enforce_eager=True,
        max_model_len=4096,
    )

    block_size = llm.scheduler.block_manager.block_size
    total_blocks = len(llm.scheduler.block_manager.blocks)
    sample_len = len(llm.tokenizer.encode(prompts[0]))

    print(f"é…ç½®:")
    print(f"  Prompté•¿åº¦: {sample_len} tokens")
    print(f"  è¾“å‡ºé•¿åº¦: {max_output} tokens")
    print(f"  Batch size: {batch_size}")
    print(f"  æ€»blocks: {total_blocks}")

    # æ·»åŠ è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output))

    # åˆ†é˜¶æ®µè®¡æ—¶
    prefill_time = 0
    prefill_tokens = 0
    decode_time = 0
    decode_tokens = 0

    start = time.time()
    step_count = 0

    while not llm.is_finished():
        step_start = time.time()
        outputs, num_tokens = llm.step(apply_compression=False)
        step_time = time.time() - step_start

        if num_tokens > 0:  # prefill
            prefill_time += step_time
            prefill_tokens += num_tokens
        else:  # decode
            decode_time += step_time
            decode_tokens += (-num_tokens)

        step_count += 1

        # è¿›åº¦æŠ¥å‘Š
        if step_count % 100 == 0:
            elapsed = time.time() - start
            print(f"  Step {step_count}: elapsed={elapsed:.1f}s, "
                  f"prefill={prefill_tokens}, decode={decode_tokens}")

    no_compress_total_time = time.time() - start
    no_compress_prefill_throughput = prefill_tokens / prefill_time if prefill_time > 0 else 0
    no_compress_decode_throughput = decode_tokens / decode_time if decode_time > 0 else 0
    no_compress_total_throughput = (prefill_tokens + decode_tokens) / no_compress_total_time

    print(f"\næ— å‹ç¼©ç»“æœ:")
    print(f"  Prefill: {prefill_tokens} tokens, {prefill_time:.2f}s, {no_compress_prefill_throughput:.0f} tok/s")
    print(f"  Decode:  {decode_tokens} tokens, {decode_time:.2f}s, {no_compress_decode_throughput:.0f} tok/s")
    print(f"  Total:   {no_compress_total_throughput:.0f} tok/s")
    print(f"  Decodeå æ¯”: {decode_time/no_compress_total_time*100:.1f}%")

    del llm
    clear_gpu()

    # =====================================================
    # æµ‹è¯•2ï¼šæœ‰å‹ç¼©
    # =====================================================
    print("\n--- æœ‰å‹ç¼© ---")

    llm = LlavaLLM(
        model_path,
        compressor_path=compressor_path,
        enable_compression=True,
        async_compression=False,
        compression_factor=5,
        enforce_eager=True,
        max_model_len=4096,
    )

    compressed_prompt_len = sample_len // 5
    print(f"å‹ç¼©åé…ç½®:")
    print(f"  å‹ç¼©åPrompté•¿åº¦: ~{compressed_prompt_len} tokens")
    print(f"  ç†è®ºattentionè®¡ç®—å‡å°‘: {sample_len / compressed_prompt_len:.1f}x")

    # æ·»åŠ è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output))

    # åˆ†é˜¶æ®µè®¡æ—¶
    prefill_time = 0
    prefill_tokens = 0
    compress_overhead = 0
    decode_time = 0
    decode_tokens = 0

    start = time.time()
    step_count = 0

    while not llm.is_finished():
        step_start = time.time()
        outputs, num_tokens = llm.step(apply_compression=True)
        step_time = time.time() - step_start

        if num_tokens > 0:  # prefill (åŒ…å«å‹ç¼©æ—¶é—´)
            prefill_time += step_time
            prefill_tokens += num_tokens
        else:  # decode
            decode_time += step_time
            decode_tokens += (-num_tokens)

        step_count += 1

        if step_count % 100 == 0:
            elapsed = time.time() - start
            print(f"  Step {step_count}: elapsed={elapsed:.1f}s, "
                  f"prefill={prefill_tokens}, decode={decode_tokens}")

    compress_total_time = time.time() - start
    compress_prefill_throughput = prefill_tokens / prefill_time if prefill_time > 0 else 0
    compress_decode_throughput = decode_tokens / decode_time if decode_time > 0 else 0
    compress_total_throughput = (prefill_tokens + decode_tokens) / compress_total_time

    print(f"\næœ‰å‹ç¼©ç»“æœ:")
    print(f"  Prefill: {prefill_tokens} tokens, {prefill_time:.2f}s, {compress_prefill_throughput:.0f} tok/s")
    print(f"  Decode:  {decode_tokens} tokens, {decode_time:.2f}s, {compress_decode_throughput:.0f} tok/s")
    print(f"  Total:   {compress_total_throughput:.0f} tok/s")
    print(f"  Decodeå æ¯”: {decode_time/compress_total_time*100:.1f}%")

    del llm
    clear_gpu()

    # =====================================================
    # å¯¹æ¯”åˆ†æ
    # =====================================================
    print("\n" + "=" * 70)
    print(" å¯¹æ¯”åˆ†æ")
    print("=" * 70)

    prefill_speedup = compress_prefill_throughput / no_compress_prefill_throughput if no_compress_prefill_throughput > 0 else 0
    decode_speedup = compress_decode_throughput / no_compress_decode_throughput if no_compress_decode_throughput > 0 else 0
    total_speedup = compress_total_throughput / no_compress_total_throughput if no_compress_total_throughput > 0 else 0

    print(f"\nPrefillååå˜åŒ–: {(prefill_speedup-1)*100:+.1f}%")
    print(f"Decodeååå˜åŒ–:  {(decode_speedup-1)*100:+.1f}%")
    print(f"")
    print(f"æ— å‹ç¼©æ€»åå: {no_compress_total_throughput:.0f} tok/s")
    print(f"æœ‰å‹ç¼©æ€»åå: {compress_total_throughput:.0f} tok/s")
    print(f"")

    improvement = (total_speedup - 1) * 100
    print(f"ğŸ¯ æ€»ååæå‡: {improvement:+.1f}%")

    if improvement >= 30:
        print(f"âœ… æˆåŠŸï¼å‹ç¼©å¸¦æ¥ {improvement:.1f}% ååæå‡ (è¶…è¿‡30%ç›®æ ‡)")
    elif improvement >= 0:
        print(f"âš ï¸ ååæå‡ {improvement:.1f}% (æœªè¾¾åˆ°30%ç›®æ ‡)")
    else:
        print(f"âŒ ååä¸‹é™ {-improvement:.1f}%")

    return {
        'batch_size': batch_size,
        'prompt_tokens': prompt_tokens,
        'max_output': max_output,
        'no_compress_throughput': no_compress_total_throughput,
        'compress_throughput': compress_total_throughput,
        'improvement': improvement,
        'decode_speedup': (decode_speedup - 1) * 100
    }


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='/data/huggingface/llava-1.5-7b-hf')
    parser.add_argument('--compressor', default=str(CKPT_DIR / "llava_mlp.pth"))
    parser.add_argument('--bs', type=int, default=128, help='Batch size')
    parser.add_argument('--prompt', type=int, default=1000, help='Prompt tokens')
    parser.add_argument('--output', type=int, default=512, help='Max output tokens')
    args = parser.parse_args()

    print("#" * 70)
    print(" Decodeé˜¶æ®µåŠ é€Ÿæµ‹è¯• - ç›®æ ‡: å‹ç¼©ååè¶…è¿‡30%")
    print("#" * 70)

    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {'batch_size': args.bs, 'prompt_tokens': args.prompt, 'max_output': args.output},
    ]

    results = []
    for config in configs:
        result = test_decode_speedup(
            args.model,
            args.compressor,
            **config
        )
        results.append(result)

    # æ€»ç»“
    print("\n" + "#" * 70)
    print(" æµ‹è¯•æ€»ç»“")
    print("#" * 70)

    for r in results:
        print(f"\nBS={r['batch_size']}, Prompt={r['prompt_tokens']}, Output={r['max_output']}:")
        print(f"  æ€»ååæå‡: {r['improvement']:+.1f}%")
        print(f"  DecodeåŠ é€Ÿ: {r['decode_speedup']:+.1f}%")


if __name__ == '__main__':
    main()
