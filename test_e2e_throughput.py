#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯ååæµ‹è¯•ï¼šéªŒè¯å‹ç¼©åœ¨å¤§batché•¿promptä¸‹çš„30%+ååæå‡
============================================================

å…³é”®å‘ç°ï¼š
- åœ¨BS=256, Prompt=1258æ—¶ï¼ŒDecodeååæå‡101%
- éœ€è¦ç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯æ€»ååèƒ½å¦è¶…è¿‡30%

é…ç½®ç­–ç•¥ï¼š
- å¤§batch size (256)
- é•¿prompt (è®©attentionæˆä¸ºç“¶é¢ˆ)
- ä¸­ç­‰è¾“å‡º (è®©decodeå ä¸»å¯¼æ—¶é—´)

"""

from fastcache_paths import ensure_sys_paths, CKPT_DIR, DATASETS_DIR, RESULTS_DIR

ensure_sys_paths()

import os
import sys

import torch
import gc
import time
from typing import List


def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


def generate_prompts(num: int, target_tokens: int) -> List[str]:
    base = "USER: Please explain "
    topics = ["AI", "ML", "DL", "NLP", "CV", "RL", "robotics", "quantum", "blockchain", "cloud"]
    expansion = " in great detail covering history, applications, challenges, and future. "

    prompts = []
    repeat = target_tokens // 30
    for i in range(num):
        topic = topics[i % len(topics)]
        prompt = base + topic + expansion * repeat + " ASSISTANT:"
        prompts.append(prompt)
    return prompts


def test_e2e_throughput(
    model_path: str,
    compressor_path: str,
    batch_size: int = 256,
    prompt_tokens: int = 2500,
    max_output: int = 256
):
    """
    ç«¯åˆ°ç«¯ååæµ‹è¯•
    """
    from nanovllm.sampling_params import SamplingParams
    from nanovllm.engine.llava_engine import LlavaLLM

    print("\n" + "=" * 70)
    print(f" ç«¯åˆ°ç«¯æµ‹è¯•: BS={batch_size}, Promptâ‰ˆ{prompt_tokens}, Output={max_output}")
    print("=" * 70)

    prompts = generate_prompts(batch_size, prompt_tokens)

    # =====================================================
    # æµ‹è¯•1ï¼šæ— å‹ç¼©
    # =====================================================
    print("\n--- æ— å‹ç¼©åŸºçº¿ ---")
    clear_gpu()

    llm = LlavaLLM(
        model_path,
        enable_compression=False,
        enforce_eager=True,
        max_model_len=4096,
    )

    sample_len = len(llm.tokenizer.encode(prompts[0]))
    print(f"å®é™…Prompté•¿åº¦: {sample_len} tokens")

    # æ·»åŠ è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output, temperature=1.0))

    # è¿è¡Œ
    prefill_time = 0
    prefill_tokens = 0
    decode_time = 0
    decode_steps = 0

    start = time.time()

    while not llm.is_finished():
        step_start = time.time()
        outputs, num_tokens = llm.step(apply_compression=False)
        step_time = time.time() - step_start

        if num_tokens > 0:
            prefill_time += step_time
            prefill_tokens += num_tokens
        else:
            decode_time += step_time
            decode_steps += 1

    no_compress_total_time = time.time() - start
    no_compress_total_tokens = prefill_tokens + decode_steps * batch_size
    no_compress_throughput = no_compress_total_tokens / no_compress_total_time

    print(f"\næ— å‹ç¼©ç»“æœ:")
    print(f"  Prefill: {prefill_tokens} tokens, {prefill_time:.2f}s")
    print(f"  Decode: {decode_steps} steps, {decode_time:.2f}s")
    print(f"  æ€»æ—¶é—´: {no_compress_total_time:.2f}s")
    print(f"  æ€»åå: {no_compress_throughput:.0f} tok/s")
    print(f"  Decodeå æ¯”: {decode_time/no_compress_total_time*100:.1f}%")

    del llm
    clear_gpu()

    # =====================================================
    # æµ‹è¯•2ï¼šæœ‰å‹ç¼©
    # =====================================================
    print("\n--- æœ‰å‹ç¼© ---")

    llm = LlavaLLM(
        model_path,
        compressor_path=compressor_path,
        enable_compression=True,
        async_compression=False,
        compression_factor=5,
        enforce_eager=True,
        max_model_len=4096,
    )

    print(f"å‹ç¼©åPrompté•¿åº¦: ~{sample_len//5} tokens")

    # æ·»åŠ è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=max_output, temperature=1.0))

    # è¿è¡Œï¼ˆè¿™æ¬¡ç”¨stepçš„apply_compression=Trueï¼‰
    prefill_time = 0
    prefill_tokens = 0
    decode_time = 0
    decode_steps = 0

    start = time.time()

    while not llm.is_finished():
        step_start = time.time()
        outputs, num_tokens = llm.step(apply_compression=True)
        step_time = time.time() - step_start

        if num_tokens > 0:
            prefill_time += step_time
            prefill_tokens += num_tokens
        else:
            decode_time += step_time
            decode_steps += 1

    compress_total_time = time.time() - start
    compress_total_tokens = prefill_tokens + decode_steps * batch_size
    compress_throughput = compress_total_tokens / compress_total_time

    print(f"\næœ‰å‹ç¼©ç»“æœ:")
    print(f"  Prefill+å‹ç¼©: {prefill_tokens} tokens, {prefill_time:.2f}s")
    print(f"  Decode: {decode_steps} steps, {decode_time:.2f}s")
    print(f"  æ€»æ—¶é—´: {compress_total_time:.2f}s")
    print(f"  æ€»åå: {compress_throughput:.0f} tok/s")
    print(f"  Decodeå æ¯”: {decode_time/compress_total_time*100:.1f}%")

    del llm
    clear_gpu()

    # =====================================================
    # å¯¹æ¯”
    # =====================================================
    print("\n" + "=" * 70)
    print(" æœ€ç»ˆå¯¹æ¯”")
    print("=" * 70)

    speedup = compress_throughput / no_compress_throughput
    improvement = (speedup - 1) * 100

    print(f"\næ— å‹ç¼©åå: {no_compress_throughput:.0f} tok/s")
    print(f"æœ‰å‹ç¼©åå: {compress_throughput:.0f} tok/s")
    print(f"")
    print(f"ğŸ¯ æ€»ååæå‡: {improvement:+.1f}%")

    if improvement >= 30:
        print(f"\nâœ… æˆåŠŸï¼å‹ç¼©å¸¦æ¥ {improvement:.1f}% ååæå‡ (è¶…è¿‡30%ç›®æ ‡)")
    elif improvement >= 0:
        print(f"\nâš ï¸ ååæå‡ {improvement:.1f}% (æœªè¾¾åˆ°30%ç›®æ ‡)")
    else:
        print(f"\nâŒ ååä¸‹é™ {-improvement:.1f}%")

    return {
        'batch_size': batch_size,
        'prompt_tokens': sample_len,
        'max_output': max_output,
        'no_compress_throughput': no_compress_throughput,
        'compress_throughput': compress_throughput,
        'improvement': improvement
    }


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='/data/huggingface/llava-1.5-7b-hf')
    parser.add_argument('--compressor', default=str(CKPT_DIR / "llava_mlp.pth"))
    parser.add_argument('--bs', type=int, default=256)
    parser.add_argument('--prompt', type=int, default=2500)
    parser.add_argument('--output', type=int, default=256)
    args = parser.parse_args()

    print("#" * 70)
    print(" ç«¯åˆ°ç«¯ååæµ‹è¯• - ç›®æ ‡: è¶…è¿‡30%ååæå‡")
    print("#" * 70)

    result = test_e2e_throughput(
        args.model,
        args.compressor,
        batch_size=args.bs,
        prompt_tokens=args.prompt,
        max_output=args.output
    )

    print("\n" + "#" * 70)
    print(f" æœ€ç»ˆç»“æœ: {result['improvement']:+.1f}% ååæå‡")
    print("#" * 70)


if __name__ == '__main__':
    main()
