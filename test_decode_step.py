#!/usr/bin/env python3
"""
ç²¾ç¡®æµ‹é‡å•æ¬¡decode stepçš„æ—¶é—´
============================

é—®é¢˜è¯Šæ–­ï¼šä¸ºä»€ä¹ˆå‹ç¼©ådecodeåè€Œæ›´æ…¢ï¼Ÿ

"""

from fastcache_paths import ensure_sys_paths, CKPT_DIR, DATASETS_DIR, RESULTS_DIR

ensure_sys_paths()

import os
import sys

import torch
import gc
import time
from typing import List


def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


def generate_prompts(num: int, target_tokens: int) -> List[str]:
    base = "USER: Please explain "
    topics = ["AI", "ML", "DL", "NLP", "CV", "RL", "robotics", "quantum", "blockchain", "cloud"]
    expansion = " in great detail covering history, applications, challenges, and future. "

    prompts = []
    repeat = target_tokens // 30
    for i in range(num):
        topic = topics[i % len(topics)]
        prompt = base + topic + expansion * repeat + " ASSISTANT:"
        prompts.append(prompt)
    return prompts


def measure_decode_step_time(
    model_path: str,
    compressor_path: str,
    batch_size: int = 128,
    prompt_tokens: int = 1000,
    num_decode_steps: int = 100
):
    """
    ç²¾ç¡®æµ‹é‡decode stepæ—¶é—´

    æµç¨‹ï¼š
    1. Prefillæ‰€æœ‰åºåˆ—
    2. å¦‚æœå¯ç”¨å‹ç¼©ï¼Œæ‰§è¡Œå‹ç¼©
    3. ç²¾ç¡®æµ‹é‡æ¯ä¸ªdecode stepçš„æ—¶é—´
    """
    from nanovllm.sampling_params import SamplingParams
    from nanovllm.engine.llava_engine import LlavaLLM

    print("\n" + "=" * 70)
    print(f" Decode Stepç²¾ç¡®æµ‹é‡: BS={batch_size}, Prompt={prompt_tokens}")
    print("=" * 70)

    prompts = generate_prompts(batch_size, prompt_tokens)

    # =====================================================
    # æµ‹è¯•1ï¼šæ— å‹ç¼©
    # =====================================================
    print("\n--- æ— å‹ç¼© ---")
    clear_gpu()

    llm = LlavaLLM(
        model_path,
        enable_compression=False,
        enforce_eager=True,
        max_model_len=4096,
    )

    sample_len = len(llm.tokenizer.encode(prompts[0]))
    print(f"Prompté•¿åº¦: {sample_len} tokens")

    # æ·»åŠ è¯·æ±‚ï¼ˆä½¿ç”¨å¤§çš„max_tokensç¡®ä¿ä¸ä¼šæå‰åœæ­¢ï¼‰
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=num_decode_steps + 10, temperature=1.0))

    # Prefillé˜¶æ®µ
    prefill_steps = 0
    while True:
        seqs, is_prefill = llm.scheduler.schedule()
        if not is_prefill:
            break
        llm.model_runner.run(seqs, is_prefill, apply_compression=False)
        llm.scheduler.postprocess(seqs, [1] * len(seqs))  # dummy tokens
        prefill_steps += 1

    print(f"Prefillå®Œæˆ ({prefill_steps} steps)")

    # è·å–å½“å‰context_lens
    running_seqs = list(llm.scheduler.running)
    if running_seqs:
        context_len = running_seqs[0].kv_cache_len if hasattr(running_seqs[0], 'kv_cache_len') else len(running_seqs[0])
        print(f"åˆå§‹context_len: {context_len}")

    # æµ‹é‡decodeæ­¥éª¤
    decode_times = []
    torch.cuda.synchronize()

    for step in range(num_decode_steps):
        seqs, is_prefill = llm.scheduler.schedule()
        if is_prefill or len(seqs) == 0:
            break

        torch.cuda.synchronize()
        start = time.perf_counter()

        token_ids = llm.model_runner.run(seqs, is_prefill=False, apply_compression=False)
        llm.scheduler.postprocess(seqs, token_ids)

        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        decode_times.append(elapsed)

    avg_decode_time = sum(decode_times) / len(decode_times) if decode_times else 0
    no_compress_throughput = batch_size / avg_decode_time if avg_decode_time > 0 else 0

    print(f"Decode steps: {len(decode_times)}")
    print(f"å¹³å‡decodeæ—¶é—´: {avg_decode_time*1000:.2f} ms")
    print(f"Decodeåå: {no_compress_throughput:.0f} tok/s")

    # æœ€ç»ˆcontext_len
    running_seqs = list(llm.scheduler.running)
    if running_seqs:
        final_context_len = running_seqs[0].kv_cache_len if hasattr(running_seqs[0], 'kv_cache_len') else len(running_seqs[0])
        print(f"æœ€ç»ˆcontext_len: {final_context_len}")

    del llm
    clear_gpu()

    # =====================================================
    # æµ‹è¯•2ï¼šæœ‰å‹ç¼©
    # =====================================================
    print("\n--- æœ‰å‹ç¼© ---")

    llm = LlavaLLM(
        model_path,
        compressor_path=compressor_path,
        enable_compression=True,
        async_compression=False,
        compression_factor=5,
        enforce_eager=True,
        max_model_len=4096,
    )

    # æ·»åŠ è¯·æ±‚
    for prompt in prompts:
        llm.add_request(prompt, SamplingParams(max_tokens=num_decode_steps + 10, temperature=1.0))

    # Prefill + å‹ç¼©é˜¶æ®µ
    prefill_steps = 0
    while True:
        seqs, is_prefill = llm.scheduler.schedule()
        if not is_prefill:
            break
        # Prefillæ—¶åº”ç”¨å‹ç¼©
        token_ids = llm.model_runner.run(seqs, is_prefill, apply_compression=True)

        # å‹ç¼©åé‡Šæ”¾blocks
        for seq in seqs:
            compressed_len = llm.model_runner._compressed_lens.get(seq.seq_id)
            if compressed_len is not None:
                seq.kv_cache_len = compressed_len
                # é‡Šæ”¾å¤šä½™blocks
                blocks_to_keep = llm.model_runner.get_compressed_block_count(seq.seq_id)
                if blocks_to_keep > 0 and len(seq.block_table) > blocks_to_keep:
                    for block_id in seq.block_table[blocks_to_keep:]:
                        if block_id in llm.scheduler.block_manager.used_block_ids:
                            block = llm.scheduler.block_manager.blocks[block_id]
                            block.ref_count -= 1
                            if block.ref_count == 0:
                                llm.scheduler.block_manager._deallocate_block(block_id)
                    seq.block_table = seq.block_table[:blocks_to_keep]

        llm.scheduler.postprocess(seqs, [1] * len(seqs))  # dummy tokens
        prefill_steps += 1

    print(f"Prefill+å‹ç¼©å®Œæˆ ({prefill_steps} steps)")

    # è·å–å‹ç¼©åçš„context_lens
    running_seqs = list(llm.scheduler.running)
    if running_seqs:
        context_len = running_seqs[0].kv_cache_len if hasattr(running_seqs[0], 'kv_cache_len') else len(running_seqs[0])
        print(f"å‹ç¼©ååˆå§‹context_len: {context_len}")

    # æµ‹é‡decodeæ­¥éª¤
    decode_times = []
    torch.cuda.synchronize()

    for step in range(num_decode_steps):
        seqs, is_prefill = llm.scheduler.schedule()
        if is_prefill or len(seqs) == 0:
            break

        torch.cuda.synchronize()
        start = time.perf_counter()

        token_ids = llm.model_runner.run(seqs, is_prefill=False, apply_compression=False)
        llm.scheduler.postprocess(seqs, token_ids)

        torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        decode_times.append(elapsed)

    avg_decode_time_compress = sum(decode_times) / len(decode_times) if decode_times else 0
    compress_throughput = batch_size / avg_decode_time_compress if avg_decode_time_compress > 0 else 0

    print(f"Decode steps: {len(decode_times)}")
    print(f"å¹³å‡decodeæ—¶é—´: {avg_decode_time_compress*1000:.2f} ms")
    print(f"Decodeåå: {compress_throughput:.0f} tok/s")

    # æœ€ç»ˆcontext_len
    running_seqs = list(llm.scheduler.running)
    if running_seqs:
        final_context_len = running_seqs[0].kv_cache_len if hasattr(running_seqs[0], 'kv_cache_len') else len(running_seqs[0])
        print(f"æœ€ç»ˆcontext_len: {final_context_len}")

    del llm
    clear_gpu()

    # =====================================================
    # å¯¹æ¯”
    # =====================================================
    print("\n" + "=" * 70)
    print(" Decode Stepå¯¹æ¯”")
    print("=" * 70)

    speedup = compress_throughput / no_compress_throughput if no_compress_throughput > 0 else 0
    improvement = (speedup - 1) * 100

    print(f"\næ— å‹ç¼©Decodeåå: {no_compress_throughput:.0f} tok/s")
    print(f"æœ‰å‹ç¼©Decodeåå: {compress_throughput:.0f} tok/s")
    print(f"")
    print(f"ğŸ¯ Decodeååæå‡: {improvement:+.1f}%")


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='/data/huggingface/llava-1.5-7b-hf')
    parser.add_argument('--compressor', default=str(CKPT_DIR / "llava_mlp.pth"))
    parser.add_argument('--bs', type=int, default=128)
    parser.add_argument('--prompt', type=int, default=1000)
    parser.add_argument('--steps', type=int, default=50)
    args = parser.parse_args()

    print("#" * 70)
    print(" Decode Stepç²¾ç¡®æµ‹é‡")
    print("#" * 70)

    measure_decode_step_time(
        args.model,
        args.compressor,
        batch_size=args.bs,
        prompt_tokens=args.prompt,
        num_decode_steps=args.steps
    )


if __name__ == '__main__':
    main()
