#!/usr/bin/env python3
"""
å‹ç¼©æ¨¡å¼å¯¹æ¯”æµ‹è¯•
================

å¯¹æ¯”ä¸‰ç§æ¨¡å¼çš„æ€§èƒ½ï¼š
1. æ— å‹ç¼© (Baseline)
2. åŒæ­¥å‹ç¼©
3. å¼‚æ­¥å‹ç¼©

æµ‹è¯•å…³é”®æŒ‡æ ‡ï¼š
- ååé‡ (tokens/s)
- TPOT (Time Per Output Token)
- å‹ç¼©å¼€é”€

"""

from fastcache_paths import ensure_sys_paths, CKPT_DIR, DATASETS_DIR, RESULTS_DIR

ensure_sys_paths()

import os
import sys
import gc
import time
import argparse


import torch
from typing import List, Dict
from dataclasses import dataclass


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    mode: str
    total_time: float
    num_tokens: int
    throughput: float
    tpot_ms: float
    success: bool
    error: str = ""


def force_clear_gpu():
    """å¼ºåˆ¶æ¸…ç†GPUå†…å­˜"""
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"[GPU] å·²åˆ†é…: {allocated:.2f} GB")


def generate_prompts(num_prompts: int, complexity: str = "medium") -> List[str]:
    """ç”Ÿæˆæµ‹è¯•prompts"""
    if complexity == "simple":
        base = "USER: What is {}? ASSISTANT:"
        topics = ["Python", "AI", "Machine Learning", "Deep Learning", "Neural Networks",
                  "Computer Vision", "NLP", "Reinforcement Learning", "Data Science", "Statistics"]
    else:
        base = "USER: Please explain {} in detail, covering its history, current applications, and future prospects. ASSISTANT:"
        topics = [
            "artificial intelligence",
            "machine learning algorithms",
            "deep neural networks",
            "computer vision applications",
            "natural language processing",
            "reinforcement learning",
            "generative AI models",
            "data mining techniques",
            "statistical analysis methods",
            "cloud computing infrastructure"
        ]

    prompts = []
    for i in range(num_prompts):
        topic = topics[i % len(topics)]
        prompts.append(base.format(topic))

    return prompts


def run_single_test(
    mode: str,  # 'none', 'sync', 'async'
    model_path: str,
    prompts: List[str],
    max_tokens: int,
    compression_factor: int = 5
) -> TestResult:
    """è¿è¡Œå•ä¸ªæµ‹è¯•"""
    from nanovllm.sampling_params import SamplingParams
    from nanovllm.engine.llava_engine import LlavaLLM

    mode_names = {
        'none': 'æ— å‹ç¼©',
        'sync': 'åŒæ­¥å‹ç¼©',
        'async': 'å¼‚æ­¥å‹ç¼©'
    }
    mode_name = mode_names.get(mode, mode)

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•: {mode_name}")
    print(f"{'='*60}")

    force_clear_gpu()

    try:
        # é…ç½®å‚æ•°
        enable_compression = (mode != 'none')
        async_compression = (mode == 'async')

        print(f"åˆå§‹åŒ–LLM (compression={enable_compression}, async={async_compression})...")

        llm = LlavaLLM(
            model_path,
            enable_compression=enable_compression,
            async_compression=async_compression,
            compression_factor=compression_factor,
            enforce_eager=True,  # ç¦ç”¨CUDA Graphä»¥ç®€åŒ–æµ‹è¯•
            max_model_len=2048,
        )

        sampling_params = [SamplingParams(max_tokens=max_tokens)] * len(prompts)

        # é¢„çƒ­
        print("é¢„çƒ­...")
        _ = llm.generate(prompts[:1], sampling_params[:1], use_tqdm=False)
        torch.cuda.synchronize()
        force_clear_gpu()

        # æ­£å¼æµ‹è¯•
        print(f"å¼€å§‹æµ‹è¯• ({len(prompts)} prompts, max_tokens={max_tokens})...")
        start_time = time.time()
        outputs = llm.generate(prompts, sampling_params, use_tqdm=True)
        torch.cuda.synchronize()
        total_time = time.time() - start_time

        # ç»Ÿè®¡
        num_tokens = sum(len(o['token_ids']) for o in outputs)
        throughput = num_tokens / total_time if total_time > 0 else 0
        tpot = total_time / num_tokens * 1000 if num_tokens > 0 else 0

        print(f"\nç»“æœ:")
        print(f"  æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"  è¾“å‡ºtokens: {num_tokens}")
        print(f"  ååé‡: {throughput:.1f} tok/s")
        print(f"  TPOT: {tpot:.2f} ms")

        result = TestResult(
            mode=mode,
            total_time=total_time,
            num_tokens=num_tokens,
            throughput=throughput,
            tpot_ms=tpot,
            success=True
        )

        del llm
        force_clear_gpu()

        return result

    except Exception as e:
        import traceback
        print(f"âœ— {mode_name}æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

        force_clear_gpu()

        return TestResult(
            mode=mode,
            total_time=0,
            num_tokens=0,
            throughput=0,
            tpot_ms=0,
            success=False,
            error=str(e)
        )


def print_comparison(results: List[TestResult]):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n")
    print("=" * 80)
    print(" æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    successful = [r for r in results if r.success]
    if not successful:
        print("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•")
        return

    mode_names = {
        'none': 'æ— å‹ç¼© (Baseline)',
        'sync': 'åŒæ­¥å‹ç¼©',
        'async': 'å¼‚æ­¥å‹ç¼©'
    }

    print(f"{'æ¨¡å¼':<22} {'æ—¶é—´(s)':<10} {'ååé‡(tok/s)':<16} {'TPOT(ms)':<12} {'å¯¹æ¯”åŸºå‡†':<15}")
    print("-" * 80)

    baseline = next((r for r in successful if r.mode == 'none'), None)

    for r in successful:
        name = mode_names.get(r.mode, r.mode)

        comparison = ""
        if baseline and r.mode != 'none':
            ratio = r.throughput / baseline.throughput
            if ratio >= 1:
                comparison = f"+{(ratio-1)*100:.1f}%"
            else:
                comparison = f"{(ratio-1)*100:.1f}%"

        print(f"{name:<22} {r.total_time:<10.3f} {r.throughput:<16.1f} {r.tpot_ms:<12.2f} {comparison:<15}")

    print("=" * 80)

    # åˆ†æ
    if baseline:
        print("\nğŸ“Š åˆ†æ:")

        sync_result = next((r for r in successful if r.mode == 'sync'), None)
        async_result = next((r for r in successful if r.mode == 'async'), None)

        if sync_result:
            overhead = (baseline.throughput - sync_result.throughput) / baseline.throughput * 100
            print(f"  åŒæ­¥å‹ç¼©å¼€é”€: {overhead:.1f}%")

        if async_result:
            overhead = (baseline.throughput - async_result.throughput) / baseline.throughput * 100
            print(f"  å¼‚æ­¥å‹ç¼©å¼€é”€: {overhead:.1f}%")

        if sync_result and async_result:
            improvement = (async_result.throughput - sync_result.throughput) / sync_result.throughput * 100
            print(f"  å¼‚æ­¥ vs åŒæ­¥: {improvement:+.1f}%")

        # å‹ç¼©æ”¶ç›Šåˆ†æ
        print("\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
        if async_result and baseline:
            if async_result.throughput > baseline.throughput * 0.9:
                print(f"  âœ“ å¼‚æ­¥å‹ç¼©å¼€é”€å¾ˆå°ï¼ˆ<10%ï¼‰ï¼Œå‹ç¼©æ¥è¿‘é›¶å¼€é”€ï¼")
            elif async_result.throughput > baseline.throughput * 0.8:
                print(f"  â—‹ å¼‚æ­¥å‹ç¼©å¼€é”€é€‚ä¸­ï¼ˆ10-20%ï¼‰")
            else:
                print(f"  âš  å¼‚æ­¥å‹ç¼©å¼€é”€è¾ƒå¤§ï¼ˆ>20%ï¼‰ï¼Œéœ€ä¼˜åŒ–")


def main():
    parser = argparse.ArgumentParser(description='å‹ç¼©æ¨¡å¼å¯¹æ¯”æµ‹è¯•')
    parser.add_argument('--model', type=str,
                        default='/data/huggingface/llava-1.5-7b-hf',
                        help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--num_prompts', type=int, default=8,
                        help='æµ‹è¯•promptsæ•°é‡')
    parser.add_argument('--max_tokens', type=int, default=128,
                        help='æœ€å¤§è¾“å‡ºtokens')
    parser.add_argument('--compression_factor', type=int, default=5,
                        help='å‹ç¼©å› å­')
    parser.add_argument('--skip_baseline', action='store_true',
                        help='è·³è¿‡æ— å‹ç¼©åŸºå‡†æµ‹è¯•')
    parser.add_argument('--modes', type=str, default='all',
                        choices=['all', 'none', 'sync', 'async', 'compress'],
                        help='æµ‹è¯•æ¨¡å¼')
    args = parser.parse_args()

    print("#" * 80)
    print(" å‹ç¼©æ¨¡å¼æ€§èƒ½å¯¹æ¯”")
    print("#" * 80)
    print(f"æ¨¡å‹: {args.model}")
    print(f"Prompts: {args.num_prompts}")
    print(f"Max tokens: {args.max_tokens}")
    print(f"å‹ç¼©å› å­: {args.compression_factor}")

    # ç”Ÿæˆprompts
    prompts = generate_prompts(args.num_prompts, "medium")

    results = []

    # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å¼
    if args.modes == 'all':
        modes_to_test = ['none', 'sync', 'async'] if not args.skip_baseline else ['sync', 'async']
    elif args.modes == 'compress':
        modes_to_test = ['sync', 'async']
    else:
        modes_to_test = [args.modes]

    # è¿è¡Œæµ‹è¯•
    for mode in modes_to_test:
        result = run_single_test(
            mode=mode,
            model_path=args.model,
            prompts=prompts,
            max_tokens=args.max_tokens,
            compression_factor=args.compression_factor
        )
        results.append(result)

    # æ‰“å°å¯¹æ¯”
    print_comparison(results)

    print("\næµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    main()
